# -*- coding: utf-8 -*-
"""Decision Tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zQXVHRjy80aARZlLFmPreAGcNHhLVvzr

# ***CSE445 Machine Learning***
# ***Online Class Preference Prediction Using Machine Learning Approach***

###***Notebook owner:*** *Nazmul Hasan | Github: https://github.com/NazmulHasan7*

*We are proposing a machine-learning
model to predict preference of online class among
Bangladeshi students. Our goal is to create an efficient
machine-learning model to predict if a student prefers
online class or not by using some common available
features such as age, gender, level of study, preferred
device, results, knowledge and class performance
development during online class, internet availability,
location of joining, difficulties faced, etc.*

## ***Methodology***

*The major objective of this work is to develop a
machine-learning model that will aid to predict if a
student likes online classes or not. The approach
adopted in this work is outlined in Fig. 1*



![test.drawio.svg](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<!-- Do not edit this file with editors other than diagrams.net -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="571px" height="211px" viewBox="-0.5 -0.5 571 211" content="&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2022-08-04T05:17:53.464Z&quot; agent=&quot;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36&quot; etag=&quot;T4CV0mCGPS2oxVbYxXhV&quot; version=&quot;20.0.3&quot; type=&quot;device&quot;&gt;&lt;diagram id=&quot;qTEaN63Ci5Y_evzGbHFM&quot; name=&quot;Page-1&quot;&gt;7VrbbuM2EP0aAe1DCl1M2XlM7N1uUbQI6gLdPDISLROVRC1Fr+18fXmVKNHeqKkWFmq/JOTMcEieOTORRvGiZXH4mcJq+xtJUe6FfnrwopUXhkHgL/gvITkqyQz4SpBRnGqjVrDGr0gLjdkOp6juGDJCcoarrjAhZYkS1pFBSsm+a7YheXfXCmbIEawTmLvSv3DKtuZe8X2r+IRwttVbL8K5UhTQGOub1FuYkr0lij540ZISwtSoOCxRLsAzuKh1H89om4NRVLIhC9az1/0fu1+jPWK/bNYY1E/L1zvt5SvMd/rCT5S85Kjwwjjnbh9fKB9lYvSR0GKXQ4ZJqS/EjgYlSnZlisRGPrfcbzFD6womQrvnvOCyLStyPgv4UG+JKEOHs3cJGoQ4tRApEKNHbqIXhJEGVbMqMKzaWzEywG+t8MRaBjUtssZ1ixwfaPD+BZChA+QKMuii+JB82eEaTwLFaDE1FKOBKD5RdFdRkqC6xmVm9HUFyw6i8ZedyK7HhOSEetEDV9LsBf7Aj8/P5/d+/ShR5SWiZHcbWOD8qJYUpCS1ikOjr2WhElq/Oig5jwO7gznOSqWoGaRMqswxzOm9EPQrJuAgCaksB83MgAYkbFyyEmMRESBwAhzrt2yDxtYQ411uwtaNCk6jUQECMgRcKIME5EXFrEN340EFqPFgaC8EvlxlUV8IJfmFXNJfSAI5tc6k0mDIrRtVc+E2MYBIjcZSpodxc7Q8AEuuUqXVWaialGmUsaXjqdPIM+sA/XjIaRMUW9ilirZzOKUygyejSg5DwAtXHjC5+j1zKs+6yjFjsrz4guQ14nDHsBBolC91pRK6V5gY4dZ/UohFIWrM1U+hQTUTyAtXFw5BPLniD84Uf9+FWTyg5TI0E0MxvDiKsYOiLgd9DCVLp4AhCMHEMJw7GH46Voi6GPJLQr7lKRWpGC7w6zSel/tPepeHeOEmO6oxFY8BfSQ1fyf2ynF5CO8dCB2QUJk+iJdgPktyyB+Wky4uXRDRAbPPWiPGz0L+E9Cz1cEyWx3NpORX+WxPrFVi2i6TM7NOHRWlztt3Lx78OmRHE/QNHExbANIMsbde0dz4WvEDJ8JnZBSJd9+v3eOeiqne4Ylg8dhp6DM/Rx/jQl1Tr7Jf4/uO+qns9xwpHBxHkmLNtd/POtOYuXrahQNpF91oNwbt3E7VddIuGki72Y12Y9Au/F60a6n2bJHwLdq1THvuEO270242kHbgRrsxaOc2Qsei3XuqXXCxagcG0i6+0W4M2rldsOukXTyQdvMb7cagndv5u07azQfSbnGj3Ri0c1ulY9FuhGe7YHq0+3/8kY39rqMoGkY7zgN4tMwqYVCfP/Bsdnqfc+cCC/Atez5QJxg3B9xW9+/is6K/5rGqN1j0Y3spIb5zd5OgZpT8jZbqI/uqJCW3fNzgPO+J9MfxVcIZirj8UTRkcQLzB60ocJqKbU62c7uppo/9Xzq6s/kZIliEvj9B6D5fRmvoBm5T/ArCEPXypPmHKSsM8Thh8PRXcyuF2u/l0Yd/AA==&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g><rect x="0" y="0" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 30px; margin-left: 1px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Problem<br />Formulation</div></div></div></foreignObject><text x="60" y="34" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Problem...</text></switch></g><rect x="150" y="0" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 30px; margin-left: 151px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Data<br />Acquisition</div></div></div></foreignObject><text x="210" y="34" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Data...</text></switch></g><rect x="300" y="0" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 30px; margin-left: 301px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Data<br />Pre-processing<span style="color: rgba(0, 0, 0, 0); font-family: monospace; font-size: 0px; text-align: start;">%3CmxGraphModel%3E%3Croot%3E%3CmxCell%20id%3D%220%22%2F%3E%3CmxCell%20id%3D%221%22%20parent%3D%220%22%2F%3E%3CmxCell%20id%3D%222%22%20value%3D%22Data%26lt%3Bbr%26gt%3BAcquisition%22%20style%3D%22rounded%3D0%3BwhiteSpace%3Dwrap%3Bhtml%3D1%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%3CmxGeometry%20x%3D%22380%22%20y%3D%22150%22%20width%3D%22120%22%20height%3D%2260%22%20as%3D%22geometry%22%2F%3E%3C%2FmxCell%3E%3C%2Froot%3E%3C%2FmxGraphModel%3E</span></div></div></div></foreignObject><text x="360" y="34" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Data...</text></switch></g><rect x="450" y="0" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 30px; margin-left: 451px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Splitting Dataset <br />to Train &amp; Test set</div></div></div></foreignObject><text x="510" y="34" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Splitting Dataset...</text></switch></g><rect x="450" y="100" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 130px; margin-left: 451px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Data <br />Scalling</div></div></div></foreignObject><text x="510" y="134" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Data...</text></switch></g><rect x="295" y="100" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 130px; margin-left: 296px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Model<br />Training</div></div></div></foreignObject><text x="355" y="134" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Model...</text></switch></g><rect x="150" y="100" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 130px; margin-left: 151px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Hyper<br />parameter<br />optimization</div></div></div></foreignObject><text x="210" y="134" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Hyper...</text></switch></g><rect x="0" y="100" width="120" height="60" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 130px; margin-left: 1px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Desired<br />Model</div></div></div></foreignObject><text x="60" y="134" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Desired...</text></switch></g><path d="M 120 30 L 143.63 30" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 148.88 30 L 141.88 33.5 L 143.63 30 L 141.88 26.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 270 30 L 293.63 30" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 298.88 30 L 291.88 33.5 L 293.63 30 L 291.88 26.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 420 30 L 443.63 30" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 448.88 30 L 441.88 33.5 L 443.63 30 L 441.88 26.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 510 60 L 510 93.63" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 510 98.88 L 506.5 91.88 L 510 93.63 L 513.5 91.88 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 450 130 L 421.37 130" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 416.12 130 L 423.12 126.5 L 421.37 130 L 423.12 133.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 295 130 L 276.37 130" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 271.12 130 L 278.12 126.5 L 276.37 130 L 278.12 133.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 150 130 L 126.37 130" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 121.12 130 L 128.12 126.5 L 126.37 130 L 128.12 133.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 210 160 L 210 180 L 355 180 L 355 166.37" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 355 161.12 L 358.5 168.12 L 355 166.37 L 351.5 168.12 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><rect x="240" y="180" width="90" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 88px; height: 1px; padding-top: 195px; margin-left: 241px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Not Satisfied</div></div></div></foreignObject><text x="285" y="199" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Not Satisfied</text></switch></g><rect x="110" y="70" width="60" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 85px; margin-left: 111px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Satisfied</div></div></div></foreignObject><text x="140" y="89" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Satisfied</text></switch></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.diagrams.net/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Text is not SVG - cannot display</text></a></switch></svg>)

*The model to be developed to predict the response for
the training data will be developed using the decision
tree technique. It is one of the most popular and
straightforward machine learning algorithms for
categorization problems. Since supervised
learning approach is to be used in this work and the
model has to predict a target class that is categorized
into “Yes” and “No”, the decision tree algorithm will
be useful to create a training model that can predict the
target class by learning some decision rules inferred
from training data.*

## ***Importing Libraries***
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import pandas as pd # data processing
import numpy as np # linear algebra
import matplotlib.pyplot as plt # visualization
# %matplotlib inline

import seaborn as sns
# increases the size of sns plots
sns.set(rc={'figure.figsize':(8,6)})

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, auc, classification_report
import warnings
warnings.filterwarnings('ignore')

"""## ***Data Acquisition***

*Dataset is collected from Kaggle.
The dataset is created based on an online survey on
Bangladeshi students and it contains 17 features such
as age, level of study, devices used, result, knowledge
and class performance in online class, have interest,
internet availability, institute type, happy with online
class etc.* 
"""

# mount google drive 
from google.colab import drive
drive.mount('/content/drive')

# raw data in panda dataframe
df = pd.read_csv('/content/drive/MyDrive/CSE 445 Project/Online Survey Data on Education Bd.csv')
print('Data Frame Shape: \n{}'.format(df.shape))
df.columns = df.columns.str.replace('Used smartphone/computer/laptop previously before online class?',
'Used Electronic Devices?')
# shows five instances of the dataframe
print('First few instances of the dataset: ')
df.head()

# columns of the dataset
df.columns

# investigating all the elements whithin each Feature
for column in df:
  unique_vals = df[column].unique()
  nr_values = len(unique_vals)
  
  if nr_values < 10:
    print('The number of values for feature {} :{} -- {}'.format(column, nr_values,unique_vals))
  else:
    print('The number of values for feature {} :{}'.format(column, nr_values))

# checking for the null values
df.isnull().sum()

"""## ***Data Preprocessing***

*For some entries in the collection, multiple columns have null values. The null values are removed. Correlation Matrix is also plotted to see the relationship among attributes.*

### ***Removing Null Values***
*Removing null values to make a clean dataset*
"""

# removing rows containing null values and creating a demo dataset
new_df = df.dropna()
print('New Data Frame Shape: ', new_df.shape)

# checking null values in new data frame
new_df.isnull().sum()

# exporting new dataframe as csv
new_df.to_csv('/content/drive/MyDrive/CSE 445 Project/Online Education Filtered.csv')

# attributes of new dataframe
new_df.columns

"""### ***Dataset Encoding***
*Encoding the dataset to make it suitable for machine learning algorithms*
"""

# data types
new_df.dtypes

# Find out all the features with type object
objectList = new_df.select_dtypes(include = "object").columns
print (objectList)

#Label Encoding for object to numeric conversion
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

for obj in objectList:
    new_df[obj] = encoder.fit_transform(new_df[obj].astype(str))

print (new_df.info())

# exporting new dataframe as csv
new_df.to_csv('/content/drive/MyDrive/CSE 445 Project/Online Education Cleanded Dataset.csv')

"""### ***Splitting Dataset***
*Splitting the dataset in a 70:30 ratio.
70% for training & 30% for testing* 
"""

# separating attributes and target
attribute = new_df.drop(columns = ['Happy with online education?'])
target = new_df['Happy with online education?']
print('Attribute Shape: ', attribute.shape)
print('Target Shape: ', target.shape)

target.value_counts()

# first few instances of attribute
attribute.columns = attribute.columns.str.replace('Used smartphone/computer/laptop previously before online class?', 
                                                    'Used Electronic Devices?')
attribute.head()

# first few instances of target
target.head()

# train test splitting
X_train, X_test, y_train, y_test = train_test_split(attribute, target, train_size = 0.7, test_size = 0.3, random_state = 0)

print('For training: ')
print('Attribute Shape: ', X_train.shape)
print('Target Shape: ', y_train.shape)

print('\nFor testing: ')
print('Attribute Shape: ', X_test.shape)
print('Target Shape: ', y_test.shape)

"""### ***Correlation of Features***
*Finding the correlation among the features to see how they are connected. Main purpose is to find duplicate features*
"""

# using pearson correlation
plt.figure(figsize=(16, 14))
correlation = X_train.corr()
sns.heatmap(correlation, annot=True, cmap=plt.cm.CMRmap_r)
plt.show()

"""## ***Decision Tree***
*Initially building a decision tree model with a max depth 5, later we will build a random forest classification model with hyper parameter tuing*

"""

# Decision Tree Model
dtree = DecisionTreeClassifier(max_depth = 5, random_state = 1)
dtree.fit(X_train, y_train)

# Graph available in:  https://dreampuf.github.io/GraphvizOnline
import graphviz

dot_data = tree.export_graphviz(dtree, out_file='/content/drive/MyDrive/CSE 445 Project/Decision Tree.dot',
feature_names = new_df.drop('Happy with online education?', axis=1).columns,
class_names = new_df['Happy with online education?'].unique().astype(str),
filled=True, rounded=True,
special_characters=True)

graph = graphviz.Source(dot_data)

# Decision Tree generated from Graphviz
from IPython.display import Image
Image(filename='/content/drive/MyDrive/CSE 445 Project/Decision Tree.png')

"""### ***Feature Importance***"""

# Finding importance of each feature

for i, column in enumerate(new_df.drop('Happy with online education?', axis=1)):
  print('Importance of feature {}:, {:.3f}'.format(column, dtree.feature_importances_[i]))
  feature_imp = pd.DataFrame({'Variable': [column], 'Feature Importance Score': [dtree.feature_importances_[i]]})

  try:
    final_feature_imp = pd.concat([final_feature_imp, feature_imp], ignore_index = True)
  except:
    final_feature_imp = feature_imp

# Ordering the data
final_feature_imp = final_feature_imp.sort_values('Feature Importance Score', ascending = False).reset_index()
final_feature_imp

"""### ***Result From Decision Tree***"""

# Training Accuracy Of Decision Tree
print("Training Accuracy is: ", dtree.score(X_train, y_train))

# Test Accuracy Of Decision Tree
print("Testing Accuracy is: ", dtree.score(X_test, y_test))

# after applying k fold cross validation
kfold_validation = KFold(n_splits = 10)
results = cross_val_score(dtree, attribute, target, cv = kfold_validation)
print(results)
print ('\nResults = ', np.mean(results), '+/-', np.std(results))

# Confusion Matrix
# Confusion Matrix function
def plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):
  if classes is not None:
    sns.heatmap(cm, xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True, annot_kws={'size':30})
  else:
    sns.heatmap(cm, vmin=0., vmax=1.)
    
  plt.title(title)
  plt.ylabel('True label')
  plt.xlabel('Predicted label')

# prediction
y_pred = dtree.predict(X_train)

# Plotting Confusion Matrix for Training
cmatrix = confusion_matrix(y_train, y_pred)

cmatrix

cmatrix_norm = cmatrix/cmatrix.sum(axis=1)[:, np.newaxis]
plt.figure()
plot_confusion_matrix(cmatrix_norm, classes=dtree.classes_, title='Training confusion')

# Calculating False Positives (FP), False Negatives (FN), True Positives(TP), True Negatices (TN)
FP = cmatrix.sum(axis=0) - np.diag(cmatrix)
FN = cmatrix.sum(axis=1) - np.diag(cmatrix)
TP = np.diag(cmatrix)
TN = cmatrix.sum() - (FP + FN + TP)

# precision or positive predictive value
precision = TP / (TP + FP)
print('Precision per class: ', precision)

# sensitivity, recall or true predictive rate
recall = TP / (TP + FN)
print('Recall per class: ', recall)

# false positive rate
fpr = FP / (FP + TN)
print('False positive rate per class: ', fpr)

# false negative rate
fnr = FN / (TP + FN)
print('False negative rate per class: ', fnr)

# classification error
c_error = (FP + FN) / (TP + FP + FN + TN)
print('The classification error of each class: ' ,c_error)

# overall accuracy
accuracy = (TP + TN) / (TP + FP + FN + TN)
print('The accuracy of each class: ' ,accuracy)

# Averages
print('\nAverage Recall : ' ,recall.sum()/2)
print('Average Precision : ' ,precision.sum()/2)
print('Average Miss Rate : ' ,fnr.sum()/2)
print('Average Classification error : ' ,c_error.sum()/2)
print('Average accuracy : ' ,accuracy.sum()/2)

"""### ***Tuning Decision Tree***
*Tuning the decision tree and applying cross validation technique to see if we can find a better result*
"""

from random import randint
from sklearn.model_selection import RandomizedSearchCV

parameters = {
    'max_depth' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15],
    'criterion': ['gini', 'entropy']
}

tunned_tree = DecisionTreeClassifier()
# applying cross validation technique
tunned_tree_cv = RandomizedSearchCV(tunned_tree, parameters, cv=10)
tunned_tree_cv.fit(X_train, y_train)

print('Tunned Decision Tree Parameters {}'.format(tunned_tree_cv.best_params_))
print('Best score: {}'.format(tunned_tree_cv.best_score_))

"""*So far the model accuracy is not good. Lets try random forest algortihm to see if we can find a better model with better accuracy*

*We will also perform some hyper parameter tuning to get a better model*

# ***Random Forest***
*A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Takes the average of many Decision Trees via bagging.* 

*n_estmators : number of trees in a forest*

*max_depth : the maximum depth of the tree*

*max_features : maximum number of features to consider when looking for the best split*

*min_samples_split : minimum number of samples required to split an internal node*

*min_samples_leaf : minimum number of samples required to be at a leaf node*
"""

from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=300, criterion='entropy')
forest.fit(X_train, y_train)
prediction_test = forest.predict(X=X_test)

# Training Accuracy Of Random Forest
print("Training Accuracy : ", forest.score(X_train, y_train))

# Test Accuracy Of Random Forest
print("Testing Accuracy : ", forest.score(X_test, y_test))

"""*The model overfitted since we did not define any max_depth*

## ***Randomized Search CV***
*Random Search. Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain*
"""

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 20, stop = 300, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt','log2']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 100,5)]
# Minimum number of samples required to split a node
min_samples_split = [2, 3, 5, 7, 9, 10, 11, 14]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4, 6, 7, 8]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
  'max_features': max_features,
  'max_depth': max_depth,
  'min_samples_split': min_samples_split,
  'min_samples_leaf': min_samples_leaf,
  'criterion':['entropy','gini']
}
print(random_grid)

rand_forest = RandomForestClassifier()
rand_forest_randomcv = RandomizedSearchCV(estimator=rand_forest,param_distributions=random_grid,
                                          n_iter=100,cv=3,verbose=2, random_state=100,n_jobs=-1)
# fit the randomized model
rand_forest_randomcv.fit(X_train,y_train)

# best parameters
rand_forest_randomcv.best_params_

# best estimator
rand_forest_randomcv.best_estimator_

best_random_grid = rand_forest_randomcv.best_estimator_

y_pred=best_random_grid.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print("Accuracy Score {}".format(accuracy_score(y_test,y_pred)))
print("Classification report: {}".format(classification_report(y_test,y_pred)))

"""## ***Hyperparameter Tuning*** """

from itertools import product

n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200, 300, 500]
max_features = ['auto', 'sqrt', 'log2']
max_depths = [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15]

train_results = []
test_results = []

# to iterate through all possible combinations
for feature, depth in product(max_features, max_depths):
  for estimator in n_estimators:
    tunned_forest = RandomForestClassifier(n_estimators=estimator,
                                    criterion='entropy',
                                    max_features=feature,
                                    max_depth=depth,
                                    n_jobs=1,
                                    random_state=30)

    tunned_forest.fit(X_train, y_train)
    prediction_train = tunned_forest.predict(X=X_train)
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, prediction_train)
    roc_auc = auc(false_positive_rate, true_positive_rate)
    train_results.append(roc_auc)

    prediction_test = tunned_forest.predict(X=X_test)
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, prediction_test)
    roc_auc = auc(false_positive_rate, true_positive_rate)
    test_results.append(roc_auc)


    # Checking classification accuracy of each tree
    print('For n_estimators : ' ,estimator)
    print('Classification accuracy on Train set with max_features = {} and max_depth = {}: Accuracy: = {}'
        .format(feature, depth, accuracy_score(y_train, prediction_train)))
    
    print('Classification accuracy on test set with max_features = {} and max_depth = {}: Accuracy: = {}'
        .format(feature, depth, accuracy_score(y_test, prediction_test)))
    print()
    # Generating confusion matrix
    c_matrix = confusion_matrix(y_test, prediction_test)
    c_matrix_norm = c_matrix/c_matrix.sum(axis=1)[:, np.newaxis]

    #plt.figure()
    #plot_confusion_matrix(c_matrix_norm, classes=tunned_forest.classes_,
    #    title='Classification accuracy on test set with max_features = {} and max_depth = {}: Accuracy = {}'
    #                      .format(feature, depth, accuracy_score(y_test, prediction_test)))