# -*- coding: utf-8 -*-
"""K Nearest Neighbor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EvbbrMLK3irqDP3mOhGL4NoKQjDuacKL

# ***K Nearest Neighbor - KNN Classifier*** 
*The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.*

*The goal of the k-nearest neighbor algorithm is to identify the nearest neighbors of a given query point, so that we can assign a class label to that point.*

### ***Importing Libraries***
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import pandas as pd # data processing
import numpy as np # linear algebra
import matplotlib.pyplot as plt # visualization
# %matplotlib inline

import seaborn as sns
# increases the size of sns plots
sns.set(rc={'figure.figsize':(8,6)})

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, auc, classification_report
import warnings
warnings.filterwarnings('ignore')

"""## ***Data Acquisition***

"""

# mount google drive 
from google.colab import drive
drive.mount('/content/drive')

# raw data in panda dataframe
df = pd.read_csv('/content/drive/MyDrive/CSE 445 Project/Online Education Cleanded Dataset.csv')
print('Data Frame Shape: \n{}'.format(df.shape))
#df.columns = df.columns.str.replace('Used smartphone/computer/laptop previously before online class?',
#'Used Electronic Devices?')
# shows five instances of the dataframe

# drops the first column of the dataset
df = df.iloc[: , 1:]
print('First few instances of the dataset: ')
df.head()

# columns of the dataset
df.columns

"""### ***Splitting Dataset***
*Splitting the dataset in a 70:30 ratio.
70% for training & 30% for testing* 
"""

# separating attributes and target
attribute = df.drop(columns = ['Happy with online education?'])
target = df['Happy with online education?']
print('Attribute Shape: ', attribute.shape)
print('Target Shape: ', target.shape)

target.value_counts()

# train test splitting
X_train, X_test, y_train, y_test = train_test_split(attribute, target, train_size = 0.7, test_size = 0.3, random_state = 0)

print('For training: ')
print('Attribute Shape: ', X_train.shape)
print('Target Shape: ', y_train.shape)

print('\nFor testing: ')
print('Attribute Shape: ', X_test.shape)
print('Target Shape: ', y_test.shape)

"""## ***Scaling the Columns***"""

df.describe()

#the columns that require scaling include:
# Age?, Total hours of study before online education?, Total hours of study after online education?
scale_vars = ['Age?','Total hours of study before online education?','Total hours of study after online education?']
scaler = MinMaxScaler()
df[scale_vars] = scaler.fit_transform(df[scale_vars])
df.head()

"""### ***KNN Classifier***"""

X = df.drop(columns = 'Happy with online education?').values# Input features (attributes)
y = df['Happy with online education?'].values # Target vector
print('X shape: {}'.format(np.shape(X)))
print('y shape: {}'.format(np.shape(y)))

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size=0.3, random_state=0)

numNeighbors = [30, 40, 50, 70, 100, 120, 122, 125, 130, 135, 137, 140]
trainAcc = []
testAcc = []

for k in numNeighbors:
  knn = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)
  knn.fit(X_train, y_train)
  y_predTrain = knn.predict(X_train)
  y_predTest = knn.predict(X_test)

  trainAcc.append(accuracy_score(y_train, y_predTrain))
  testAcc.append(accuracy_score(y_test, y_predTest))

plt.plot(numNeighbors, trainAcc, 'ro-', numNeighbors, testAcc,'bv--')
plt.legend(['Training Accuracy','Test Accuracy'])
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy');

index = 0

for i in numNeighbors:
  print("K = ", numNeighbors[index], ", Training Accuracy = ", trainAcc[index], " Test Accuracy = ", 
        testAcc[index], " Difference = ", np.abs(trainAcc[index]-testAcc[index])*100, "%")
  index+=1