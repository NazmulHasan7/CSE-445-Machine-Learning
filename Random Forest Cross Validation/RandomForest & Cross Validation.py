# -*- coding: utf-8 -*-
"""RandomForest & CrossValidation .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R6lGsxPWoQBrV6EnYgecjKXpWeUkwpIu

### ***Importing Libraries***
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import pandas as pd # data processing
import numpy as np # linear algebra
import matplotlib.pyplot as plt # visualization
# %matplotlib inline

import seaborn as sns
# increases the size of sns plots
sns.set(rc={'figure.figsize':(8,6)})

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, roc_curve, auc, classification_report
import warnings
warnings.filterwarnings('ignore')

"""## ***Data Acquisition***

"""

# mount google drive 
from google.colab import drive
drive.mount('/content/drive')

# raw data in panda dataframe
df = pd.read_csv('/content/drive/MyDrive/CSE 445 Project/Online Education Cleanded Dataset.csv')
print('Data Frame Shape: \n{}'.format(df.shape))
#df.columns = df.columns.str.replace('Used smartphone/computer/laptop previously before online class?',
#'Used Electronic Devices?')
# shows five instances of the dataframe

# drops the first column of the dataset
df = df.iloc[: , 1:]
print('First few instances of the dataset: ')
df.head()

# columns of the dataset
df.columns

"""### ***Splitting Dataset***
*Splitting the dataset in a 70:30 ratio.
70% for training & 30% for testing* 
"""

# separating attributes and target
attribute = df.drop(columns = ['Happy with online education?'])
target = df['Happy with online education?']
print('Attribute Shape: ', attribute.shape)
print('Target Shape: ', target.shape)

# train test splitting
X_train, X_test, y_train, y_test = train_test_split(attribute, target, train_size = 0.8, test_size = 0.2, random_state = 0)

print('For training: ')
print('Attribute Shape: ', X_train.shape)
print('Target Shape: ', y_train.shape)

print('\nFor testing: ')
print('Attribute Shape: ', X_test.shape)
print('Target Shape: ', y_test.shape)

print('Train Data:\n',y_train.value_counts())
print('Test Data:\n',y_test.value_counts())

"""## ***Random Forest***"""

from sklearn.ensemble import RandomForestClassifier
# class weight {0:0.777, 1:1.402}
rand_forest = RandomForestClassifier(random_state=399, class_weight='balanced')
rand_forest.fit(X_train, y_train)
prediction_test = rand_forest.predict(X_test)
prediction_train = rand_forest.predict(X_train)

# random forest model score
print('Training Score: ',rand_forest.score(X_train, y_train))
print('Training Score: ',rand_forest.score(X_test, y_test))

"""## ***RandomizedSearchCV***"""

# Number of trees in random forest
n_estimators = [20, 50, 80, 90, 95, 100, 105, 108, 110, 140, 160, 200, 230, 255, 270, 300, 320]
# Number of features to consider at every split
max_features = ['auto', 'sqrt','log2']
# Maximum number of levels in tree
max_depth = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
# Minimum number of samples required to split a node
min_samples_split = [2, 3, 5, 7, 9, 10, 11, 12, 13, 15]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4, 6, 7, 8, 9, 10, 13, 14, 15]
# Create the random grid
random_grid = {
      'n_estimators': n_estimators,
      'max_features': max_features,
      'max_depth': max_depth,
      'min_samples_split': min_samples_split,
      'min_samples_leaf': min_samples_leaf,
      'criterion':['entropy','gini']
      }
print(random_grid)

from sklearn.model_selection import RandomizedSearchCV
rand_forest = RandomForestClassifier(class_weight='balanced', random_state=100)
rand_forest_randomcv = RandomizedSearchCV(estimator=rand_forest,
                                          param_distributions=random_grid,
                                          n_iter=100,
                                          cv=5,
                                          verbose=2, 
                                          random_state=100,
                                          n_jobs=-1)
# fit the randomized model
rand_forest_randomcv.fit(X_train,y_train)

# best parameters
rand_forest_randomcv.best_params_

best_random_grid = rand_forest_randomcv.best_estimator_
print(best_random_grid)

# Training Accuracy Of RandomForest with best parameters
print("Training Accuracy is: ", best_random_grid.score(X_train, y_train))
# Test Accuracy Accuracy Of RandomForest with best parameters
print("Testing Accuracy is: ", best_random_grid.score(X_test, y_test))

"""##***5 Fold CrossValidation***"""

import time
# applying k fold cross validation
from sklearn.model_selection import cross_val_predict

kfold_validation = KFold(n_splits = 5)
scores = ['accuracy', 'precision', 'recall']

print('-----------5 FOLD CROSS VALIDATION------------')
for score in scores:
    print('Metric : {}'.format(score))
    print('Training Score: ', end="")
    start_time = time.time()
    print(cross_val_score(best_random_grid, X_train, y_train, scoring=score, cv=kfold_validation).mean())
    print('Computation Time: {}'.format(time.time() - start_time))
    print()

    print('\nTesting Score: ', end="")
    start_time = time.time()
    print(cross_val_score(best_random_grid, X_test, y_test, scoring=score, cv=kfold_validation).mean())
    print('Computation Time: {}'.format(time.time() - start_time))
    print('----------------------------------------------')

train_accuracy = cross_val_score(best_random_grid, X_train, y_train, scoring='accuracy', cv=5)
test_accuracy = cross_val_score(best_random_grid, X_test, y_test, scoring='accuracy', cv=5)

# plotting graph
splits = [1, 2, 3, 4, 5]
plt.plot(splits, train_accuracy, 'ro-', splits, test_accuracy,'bv--')
plt.legend(['Training Accuracy','Test Accuracy'])
plt.xlabel('Number of split')
plt.ylabel('Accuracy');

"""## ***10 Fold CrossValidation***"""

import time
# applying k fold cross validation
from sklearn.model_selection import cross_val_predict
kfold_validation = KFold(n_splits = 10)

scores = ['accuracy', 'precision', 'recall']

print('-----------10 FOLD CROSS VALIDATION------------')
for score in scores:
    print('Metric : {}'.format(score))
    print('Training Score: ', end="")
    start_time = time.time()
    print(cross_val_score(best_random_grid, X_train, y_train, scoring=score, cv=kfold_validation).mean())
    print('Computation Time: {}'.format(time.time() - start_time))
    print()

    print('\nTesting Score: ', end="")
    start_time = time.time()
    print(cross_val_score(best_random_grid, X_test, y_test, scoring=score, cv=kfold_validation).mean())
    print('Computation Time: {}'.format(time.time() - start_time))
    print('----------------------------------------------')

train_accuracy = cross_val_score(best_random_grid, X_train, y_train, scoring='accuracy', cv=10)
test_accuracy = cross_val_score(best_random_grid, X_test, y_test, scoring='accuracy', cv=10)

# plotting graph
splits = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
plt.plot(splits, train_accuracy, 'ro-', splits, test_accuracy,'bv--')
plt.legend(['Training Accuracy','Test Accuracy'])
plt.xlabel('Number of split')
plt.ylabel('Accuracy');